---
title: 爬虫
date: 2024-6-07
tag:
  - 爬虫
category:
  - 后端
---

# 爬虫学习

个人笔记，仅供学习交流参考，严禁传播!!!

## urllib库使用

```python
import urllib.request
# 使用urllib来获取百度首页的源码
# 1.定义一个url，访问地址
url = 'http://www.baidu.com'
# 2.模拟浏览器向服务器发送请求 response 响应
response = urllib.request.urlopen(url)
# 3.获取响应中的页面的源码 content 内容
#read返回的是字节形式二进制数据
#二进制数据转换为字符串--解码
content = response.read().decode('utf-8')
# 4.打印数据
print(content)
```

```python
import urllib.request
url = 'http://www.baidu.com'
#模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)
#一个类型和六个方法
#response是HTTPResponse的类型

#response.read(5) 读5个字节
#response.readline() 读取一行
#response.readlines() 一次读取一行
#response.getcode() 返回状态码 200正确，没有错
#response.geturl() 返回url地址
#response.getheaders() 获取的是一个状态信息
```



### 下载

```python
import urllib.request
# 下载网页
url_page = 'http://www.baidu.com'
# url代表的是下载的路径 filename文件的名字
urllib.request.urlretrieve(url_page,'baidu.html')
# 下载图片
url_img = '图片链接'
urllib.request.urlretrieve(url=url_img,filename='图片.jpg')
# 下载视频
url_video = '视频地址(检查地址视频链接)'
urllib.request.urlretrieve(url=url_video,filename='视频.mp4')
```

## 请求对象的定制

***url的组成***

1. http/https协议

2. 主机(域名)

3. 端口号

  `http80 https443 mysql3306`

  `oracle1521 redis6479`

4. 路径 `s`

5. 参数 `wd=`

6. 锚点 `#`

```python
import urllib.request
url = 'https://www.baidu.com'
# url的组成
response = urllib.request.urlopen(url)
content = response.read().decode('utf-8')
print(content) #打印出来的东西少了?
```

获取UA(用户代理)

检查-->网络-->刷新-->查看网址-->ua

```python
url = 'https://www.baidu.com'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'
}
# 因为urlopen方法不能存储字典,所以headers不能传递进去
# 因为参数顺序的问题，不能直接写url和headers，所以需要关键字传参
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
```

## 编解码

1. get请求方式:`urllib.parse.quote()`

```python
import urllib.request
import urllib.parse
url = 'https://www.baidu.com/s?wd='
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'
}
# 将周杰伦三个字变成unicode编码的格式
# 需要依赖于urllib.parse
name = urllib.parse.quote('周杰伦')
url = url + name
# 请求对象的定制为了解决反爬的第一种手段
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
print(content)
```

2. get请求方式:`urllib.parse.urlencode()`

urlencode应用场景:多个参数的时候

```python
import urllib.request
import urllib.parse
url = 'https://www.baidu.com/s?'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'
}
data = {
    'wd':'周杰伦',
    'sex':'男'
}
url = url + urllib.parse.urlencode(data)
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
print(content)
```



3. post请求方式

post请求方式的参数，必须编码<br>

编码之后必须调用encode方法<br>

参数是放在请求对象定制的方法中<br>

### 百度翻译

有时候需要在标头里面加上cookie等数据

```python
import urllib.request
import urllib.parse
url = 'https://fanyi.baidu.com/sug'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'
}
data = {
    'kw':'spider',
}

# post请求的参数必须要进行编码
data = urllib.parse.urlencode(data).encode('utf-8')
# post的请求的参数，是不会拼接在url的后面的，而是需要放在请求对象定制的参数中
# post请求的参数，必须要进行编码,
request = urllib.request.Request(url=url,data=data,headers=headers)
# 模拟浏览器发送请求
response = urllib.request.urlopen(request)

# 获取响应的数据
content = response.read().decode('utf-8')

# 字符串-->json对象
import json
obj = json.loads(content)

print(obj)
```



## ajax的get请求

案例:豆瓣电影

```python
import urllib.request
import urllib.parse
url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&start=0&limit=20'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'
}
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
# open方法默认情况下使用的是gbk的编码，如果我们想要保存汉字，那么需要在open方法中，指定编码格式为utf-8
fp = open('double.json','w',encoding='utf-8')
fp.write(content)
fp.close()
```

前十页

## ajax的post请求































